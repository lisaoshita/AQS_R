---
title: "New Model Results and Explanations"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
# load packages
library(readr)
library(lubridate) # for working with dates
library(dplyr)
library(caret)
library(AUC)
library(openair)
library(tree) # for decision tree at bottom
library(xgboost)
library(e1071) # for SVMs
library(kableExtra)

load("/Users/lisaoshita/Desktop/repos/apcd_r/wind.event.predictor/final.results/final.workspace.svm.edited.RData")
```


# Overview

This file contains the results/explanations of building the wind event predictors on the new data with revised variables (`wind.in.range`, `wd.at.max`). All code is generally the same as in `final.results.Rmd`. The main differences lie in the training and test sets. 

Also, with the support vector machines, we discussed that the ROC plots looked incorrect. I found that `roc()` and `auc()` were working incorrectly because `svm()` was outputting the actual class predictions instead of class probabilites. The code in this file corrects that issue. 

# Setting up the data

The following code loads/formats the training and test sets. New variables include: 

* `wind.event.in.range.s1`: number of times per day that wind direction and wind speed at S1 fell within the range that corresponds with high PM10 concentrations
* `wind.event.in.range.cdf`: same as above but at CDF
* `wd.at.max.s1`: wind direction at S1 when wind speed was at maximum for the day
* `wd.at.max.cdf`: wind direction at CDF when wind speed was at maximum for the day

```{r load data, eval=FALSE}
s1.cdf.data <- read_csv("/Users/lisaoshita/Desktop/repos/apcd_r/wind.event.predictor/data/forLisa.csv",
                        col_types = list(date = "c", ws.cdf = "n", wd.cdf = "n",
                                         pm10.cdf = "n", pm10.oso = "n", wd.s1 = "n",
                                         ws.s1 = "n", year = "n")) # contains data from 2011 - 2017

s1.cdf.data <- s1.cdf.data %>%
  mutate(date = parse_date_time(date, "Ymd HMS"))

# contains cdf and S1 data up to 2014
cdf.master <- read_csv("/Users/lisaoshita/Desktop/repos/apcd_r/wind.event.predictor/data/cdf.master.csv",
                       col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n",
                                        pm10 = "n", u = "n", v = "n", year = "n", 
                                        precip = "n", s.rad = "n", a.temp = "n",
                                        rh = "n", dp = "n", s.temp = "n", height = "n",
                                        temp850 = "n", ws.max = "n", wd.max = "n",
                                        u.max = "n", v.max = "n", time = "n", dow = "n",
                                        u.s1 = "n", v.s1 = "n", u.max.s1 = "n", v.max.s1 = "n"))

cdf.master$date <- date(cdf.master$date)

# contains cdf and S1 data from 2014 - 2017
cdf.master2 <- read_csv("/Users/lisaoshita/Desktop/repos/apcd_r/wind.event.predictor/data/cdf.master.update.csv",
                        col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n", pm10 = "n",
                                        u = "n", v = "n", year = "n", precip = "n",
                                        s.rad = "n", a.temp = "n", rh = "n", dp = "n",
                                        s.temp = "n", height = "n", temp850 = "n", ws.max = "n",
                                        wd.max = "n", u.max = "n", v.max = "n", time = "n",
                                        dow = "n", u.s1 = "n", v.s1 = "n", u.max.s1 = "n",
                                        v.max.s1 = "n"))

cdf.master2$date <- date(cdf.master2$date)
```

```{r set up training data, eval=FALSE}
# train on years before 2015
training <- s1.cdf.data %>%
  mutate(date.only = lubridate::date(date)) %>%
  filter(lubridate::year(date) < 2015)

# finding wd and ws at CDF and S1 that correspond with high pm10 concentrations
# using openair package

# ==================================
# convert polar coords to cartesian
# ==================================
make.x <- function(ws, wd){
  ws*cos((90-wd)*pi/180)
}

make.y <- function(ws, wd){
  ws*sin((90-wd)*pi/180)
}

# ================================================
# function to determine if points lie w/in range:
# ================================================
wind.in.range <- function(ws, wd, range) {
  # assumes range is a two column df with "x" and "y"
  
  # assumes ws and wd in usual format, 
  # so must convert to cartesian coords.
  # define these functions again, in case they are not 
  # in environment:
  
  make.x <- function(ws, wd){
    ws*cos((90-wd)*pi/180)
  }
  
  make.y <- function(ws, wd){
    ws*sin((90-wd)*pi/180)
  }
  
  xs <- make.x(ws, wd)
  ys <- make.y(ws, wd)
  
  # test if in range
  res <- point.in.polygon(xs, ys, range$x, range$y)
  
  # return 0 if outside, 1 if inside or on edge, NA if ws or wd is missing
  res <- ifelse(res == 0, 0, 1) # see ?point.in.polygon
  res[is.na(ws) | is.na(wd)] <- NA # preserve NA's
  return(res) 
}

# ====
# CDF 
# ====
cdf.training <- training %>%
  select(date, date.only, year, ws.cdf, wd.cdf, pm10.cdf)
colnames(cdf.training) <- c("date", "date.only", "year", "ws", "wd", "pm10")

cdf.clust <- polarCluster(cdf.training, 
                          pollutant = "pm10",
                          x = "ws",
                          wd = "wd",
                          n.clusters = 2) # cluster 2 is high pm10 

# get cluster of high PM and create cartersian coordinates:
cdf.range <- cdf.clust$data %>% 
  filter(cluster == 2) %>%
  mutate(x = make.x(ws, wd)) %>%
  mutate(y = make.y(ws, wd))

# get convex hull
chull.index <- chull(cdf.range$x, cdf.range$y)
chull.index <- c(chull.index, chull.index[1])
cdf.range.chull <- cdf.range[chull.index, c("x", "y")]

# ===
# S1 
# ===
s1.training <- training %>%
  select(date, date.only, year, wd.s1, ws.s1, pm10.cdf)
colnames(s1.training) <- c("date", "date.only", "year", "wd", "ws", "pm10")

s1.clust <- polarCluster(s1.training,
                         pollutant = "pm10",
                         x = "ws",
                         wd = "wd",
                         n.clusters = 2)

# get cluster of high PM and create cartersian coordinates:
s1.range <- s1.clust$data %>% 
  filter(cluster == 2) %>%
  mutate(x = make.x(ws, wd)) %>%
  mutate(y = make.y(ws, wd))

# get convex hull
chull.index <- chull(s1.range$x, s1.range$y)
chull.index <- c(chull.index, chull.index[1])
s1.range.chull <- s1.range[chull.index, c("x", "y")]

train1 <- training %>%
  group_by(date.only) %>%
  summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE), 
            max.ws.s1 = max(ws.s1, na.rm = TRUE),
            hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
            hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
            wind.in.range.cdf = sum(wind.in.range(ws.cdf, wd.cdf, cdf.range.chull), na.rm = TRUE), 
            wind.in.range.s1 = sum(wind.in.range(ws.s1, wd.s1, s1.range.chull), na.rm = TRUE),
            wd.at.max.s1 = ifelse(is.na(hour.max.ws.s1), NA, wd.s1[which.max(ws.s1)]),
            wd.at.max.cdf = ifelse(is.na(hour.max.ws.cdf), NA, wd.s1[which.max(ws.s1)])) %>%
  mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
  mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) # %>%

# =========================================================================================

# computing pm10 avg 24 hr concentration
pm10.averages <- training %>%
  group_by(date.only) %>%
  summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
  mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))
colnames(pm10.averages)[1] <- "date"

# joining all data frames
colnames(train1)[1] <- "date"

# merge train1 with other columns in cdf.master
train1 <- train1 %>%
  left_join(pm10.averages, by = "date") %>%
  left_join(cdf.master, by = "date") %>%
  mutate(month = month(date)) %>%
  mutate(day.of.month = day(date)) %>%
  select(did.exceed, wind.in.range.cdf, max.ws.cdf, hour.max.ws.cdf, wd.at.max.cdf,
         wind.in.range.s1, max.ws.s1, hour.max.ws.s1, wd.at.max.s1, 
         precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)

# =========================================================================================

# examining missing data w/ Missingness map 
Amelia::missmap(train1) # only 4% of the training data is missing - ok to omit these rows
train1 <- na.omit(train1) 

train1$did.exceed <- as.factor(train1$did.exceed)
```

```{r format test data, eval=FALSE}
# test on years after 2015
testing <- s1.cdf.data %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(date.only = lubridate::date(date)) %>%
  filter(lubridate::year(date) >= 2015)

test <- testing %>%
  mutate(hour = lubridate::hour(date)) %>%
  group_by(date.only) %>%
  summarize(wind.in.range.cdf = sum(wind.in.range(ws.cdf, wd.cdf, cdf.range.chull), na.rm = TRUE), 
            wind.in.range.s1 = sum(wind.in.range(ws.s1, wd.s1, s1.range.chull), na.rm = TRUE),
            max.ws.cdf = max(ws.cdf, na.rm = TRUE),
            max.ws.s1 = max(ws.s1, na.rm = TRUE),
            hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, lubridate::hour(date[which.max(ws.s1)])), 
            hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, lubridate::hour(date[which.max(ws.cdf)])),
            wd.at.max.s1 = ifelse(is.na(hour.max.ws.s1), NA, wd.s1[which.max(ws.s1)]),
            wd.at.max.cdf = ifelse(is.na(hour.max.ws.cdf), NA, wd.cdf[which.max(ws.cdf)])) %>%
  mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
  mutate(max.ws.s1 = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf))

colnames(test)[1] <- "date"

# computing 24 hour average pm10 concentration
pm10.averages <- testing %>%
  group_by(date.only) %>%
  summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
  mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))

# merge test with other columns in cdf.master2
test <- test %>%
  mutate(did.exceed = pm10.averages$did.exceed) %>%
  left_join(cdf.master2, by = "date") %>%
  mutate(month = month(date)) %>%
  mutate(day.of.month = day(date)) %>%
  select(date, did.exceed, wind.in.range.cdf, max.ws.cdf, hour.max.ws.cdf, wd.at.max.cdf,
         wind.in.range.s1, max.ws.s1, hour.max.ws.s1, wd.at.max.s1, 
         precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)

# assess rows with missing data
Amelia::missmap(test) # only 2% of the training data is missing - ok to omit these rows
test <- na.omit(test) 

test$did.exceed <- as.factor(test$did.exceed)
```

```{r clear workspace, eval=FALSE}
rm(cdf.clust, cdf.range, cdf.range.chull, 
   cdf.training, pm10.averages, s1.clust, 
   s1.range, s1.range.chull, s1.training, 
   testing, training, chull.index, make.x, 
   make.y, wind.in.range)
```

# Random forest

#### Parameter tuning

```{r, eval=FALSE}
# parameter tuning set up
tune.control <- trainControl(method = "cv",
                             number = 5,
                             verboseIter = FALSE,
                             classProbs = TRUE,
                             summaryFunction = twoClassSummary) # twoClassSummary: needed for ROC/AUC metrics

grid <- expand.grid(.mtry = c(round(sqrt(ncol(train1))),
                              8, 10, 15),
                    .splitrule = "gini",
                    .min.node.size = c(5, 10, 20))
```

```{r, eval=FALSE}
set.seed(1)

# ====================
# tuning on 500 trees
# ====================
rf.tuning.500 <- train(did.exceed ~ .,
                       data = train1,
                       method = "ranger", # fast implementation of a random forest: ranger, e1071 need to be installed
                       tuneGrid = grid,
                       num.trees = 500,
                       trControl = tune.control,
                       importance = "impurity", # allows you to assess variable importance
                       metric = "ROC")
rf.tuning.500$bestTune # allows you to view the parameter values that lead to the best performance
```

```{r}
# ========
# results: 
# ========
# optimal parameters: mtry = 4, min.node.size = 20
# ROC: 0.949
knitr::kable(rf.tuning.500$results, caption = "Tuning results with 500 trees") %>%
  kable_styling(full_width = FALSE)
```

```{r, eval=FALSE}
# ====================
# tuning on 1000 trees
# ====================
rf.tuning.1000 <- train(did.exceed ~ .,
                        data = train1,
                        method = "ranger",
                        tuneGrid = grid,
                        num.trees = 1000,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")
rf.tuning.1000$bestTune
```

```{r}
# ========
# results:
# ========
# optimal parameters: mtry = 4, min.node.size = 10
# ROC: 0.9538
knitr::kable(rf.tuning.1000$results, caption = "Tuning results with 1000 trees") %>%
  kable_styling(full_width = FALSE)
```

```{r, eval=FALSE}
# ====================
# tuning on 1500 trees
# ====================
rf.tuning.1500 <- train(did.exceed ~ .,
                        data = train1,
                        method = "ranger",
                        tuneGrid = grid,
                        num.trees = 1500,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")
rf.tuning.1500$bestTune
```

```{r}
# ========
# results:
# ========
# optimal parameters: mtry = 4, min.node.size = 5
# ROC: 0.9539
knitr::kable(rf.tuning.1500$results, caption = "Tuning results with 1500 trees") %>%
  kable_styling(full_width = FALSE)
```

ROC metrics above are pretty similar, indicating that no model performs drastically better than another. Random forests with 1,000 trees and 1,500 trees achieved almost identical ROC scores. For the sake of computational speed, we'll go with the model built on 1,000 trees and the following parameter values:

```{r}
knitr::kable(rf.tuning.1000$bestTune) %>%
  kable_styling(full_width = FALSE)
plot(rf.tuning.1000)
```

Importance of variables in the model can be assessed with: 

```{r}
plot(varImp(rf.tuning.1000))
```

#### Fitting to the full training set

The following fits the random forest with the optimal parameters found in the previous step, to the full training set and tests it on the held-out set.

```{r, eval=FALSE}
# ============= Set up parameters: 
final.grid <- data.frame(.mtry = 4, # contains the optimal parameters
                         .splitrule = "gini",
                         .min.node.size = 10)

final.control <- trainControl(method = "none", # tells train() to fit the model to the full data without any partitioning
                              verboseIter = TRUE,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

# ============= Fit the model: 
set.seed(1)
rf.fit <- train(did.exceed ~ .,
                data = train1,
                method = "ranger",
                tuneGrid = final.grid,
                trControl = final.control,
                num.trees = 1000,
                importance = "impurity",
                metric = "ROC")
```

#### Assessing performance

```{r, eval=FALSE}
# ============= Compute predictions: 
rf.preds <- predict(rf.fit, newdata = test, type = "prob")

rf.preds <- rf.preds %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)
```

The model achieved a classification accuracy of `r round(mean(rf.preds$actual == rf.preds$prediction) * 100, 3)`%.

```{r, eval=FALSE}
# ROC/AUC
roc.metric <- roc(predictions = rf.preds$yes,
                  labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric <- auc(roc.metric)
```

```{r}
# sensitivity: true positive rate - ability of the model to correctly identify wind event days
# specificity: true negative rate - ability of the model to correctly identify non-wind event days
plot(roc.metric, main = paste("Random Forest  -  AUC:", round(auc.metric, 4)))
```

The following is a confusion matrix of the model's predictions, as well as calculated precision and recall. 

```{r}
# precision = 140 / (26 + 140) = 0.843
# out of all of the days that are being predicted as wind event days, 84.3% of them are actually wind event days

# recall = 140 / (60 + 140) = 0.70
# out of all of the days that truly were wind event days, 70% of them were classified correctly

table(rf.preds$prediction, rf.preds$actual)
```

# XGBoosts

#### Parameter tuning 

```{r, eval=FALSE}
# parameter tuning
xgb.tune.grid <- expand.grid(nrounds = c(500, 1000),
                             max_depth = c(3, 6, 10),
                             eta = 0.3,
                             gamma = 1,
                             min_child_weight = 1,
                             colsample_bytree = c(0.5, 0.8),
                             subsample = c(0.5, 0.8))

tune.control <- trainControl(method = "cv",
                             number = 5,
                             verboseIter = TRUE,
                             classProbs = TRUE,
                             summaryFunction = twoClassSummary)

xgb.tune <- train(did.exceed ~ .,
                  data = train1,
                  method = "xgbTree",
                  tuneGrid = xgb.tune.grid,
                  trControl = tune.control,
                  importance = "impurity",
                  metric = "ROC")
```

The following displays tuning results and variable importance. Parameters that lead to the best performance (with an ROC of 0.946) were: 

```{r}
knitr::kable(xgb.tune$bestTune) %>%
  kable_styling(full_width = FALSE)

knitr::kable(xgb.tune$results, caption = "XGBoost tuning results") %>%
  kable_styling(full_width = FALSE)

plot(xgb.tune)
```

The following assesses variable importance. Interestingly, it appears that the variables considered "important" in the random forests are not important here (e.g. `wind.in.range.cdf` is much further down in the plot than it is with the random forest).

```{r}
plot(varImp(xgb.tune))
```

#### Fitting to the full training set

```{r, eval=FALSE}
# fit to full training set
xgb.final <- train(did.exceed ~ .,
                   data = train1,
                   method = "xgbTree",
                   tuneGrid = xgb.tune$bestTune,
                   trControl = trainControl(method = "none",
                                            verboseIter = TRUE,
                                            classProbs = TRUE,
                                            summaryFunction = twoClassSummary),
                   importance = "impurity",
                   metric = "ROC")
```

#### Assessing performance

```{r, eval=FALSE}
# # assess performance
xgb.preds <- predict(xgb.final, newdata = test, type = "prob") %>%
  as.data.frame() %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(xgb.preds$actual == xgb.preds$prediction) # 0.8955533

# ROC/AUC
xgb.roc.metric <- roc(predictions = xgb.preds$yes,
                      labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
xgb.auc.metric <- auc(xgb.roc.metric)
```

The XGBoost fit to the full training set and tested on the held-out set achieved a classification accuracy of `r round(mean(xgb.preds$actual == xgb.preds$prediction)*100, 3)`%, and an ROC/AUC of: 

```{r}
plot(xgb.roc.metric, main = paste("XGBoost  -  AUC:", round(xgb.auc.metric,4)))
```

Confusion matrix of the model's predictions: 

```{r}
table(xgb.preds$prediction, xgb.preds$actual)
```

# Support vector machines

The following code tunes and fits support vector machines with polynomial and linear kernels. 

**Note on SVM predicted class probabilities:** SVMs don't directly output predictions of probability. Instead, with the function used in this file, probabilities are computed based on the observation's distance from the hyperplane (the plane that partitions the data into classes) and a logistic distribution. 

#### Polynomial kernel

##### Parameter tuning

```{r, eval=FALSE}
# tuning svm with polynomial kernel
tune.out <- tune(svm, did.exceed ~ .,
                 data = train1,
                 kernel = "polynomial",
                 degree = 2,
                 ranges = list(cost = c(.01, .1, .5, 1, 5, 7, 10)),
                 probability = TRUE)
```

```{r}
summary(tune.out)
```

##### Fitting to the full training set

```{r, eval=FALSE}
# svm with tuned parameter on full training set
svm.tuned <- svm(did.exceed ~ .,
                 data = train1,
                 kernel = "polynomial",
                 degree = 2,
                 cost = tune.out$best.parameters$cost,
                 probability = TRUE) # tuned cost parameter
```

##### Assessing performance

```{r, eval=FALSE}
# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds <- predict(svm.tuned, newdata = train1, probability = TRUE)

# classification accuracy
mean(train.preds == train1$did.exceed)

# ROC/AUC
train.roc <- roc(predictions = attr(train.preds, "probabilities")[ , 1],
                 labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc <- auc(train.roc)
```

The SVM achieved a training classification accuracy of `r round(mean(train.preds == train1$did.exceed)*100, 3)`%. Training ROC/AUC is: 

```{r}
plot(train.roc, main = paste("SVM, training metric  -  AUC:", round(train.auc, 3)))
```

```{r, eval=FALSE}
# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds <- predict(svm.tuned, newdata = test, probability = TRUE)

# classification accuracy
mean(test.preds == test$did.exceed) #  0.8738366

# ROC/AUC
test.roc <- roc(predictions = attr(test.preds, "probabilities")[ , 1],
                labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc <- auc(test.roc)
```

Tested on the held-out set, the SVM achieved a classification accuracy of `r round(mean(test.preds == test$did.exceed)*100, 3)`%. ROC/AUC on the test set was: 

```{r}
plot(test.roc, main = paste("SVM, test metric  -  AUC:", round(test.auc, 3)))
```

#### Linear kernel

##### Parameter tuning

```{r svm with linear kernel, eval=FALSE}
# tuning svm with linear kernel
tune.out.linear <- tune(svm, did.exceed ~ ., 
                        data = train1,
                        kernel = "linear",
                        ranges = list(cost = c(0.001, .01, .1, .5, 1, 5, 7, 10)),
                        probability = TRUE)
```

```{r}
summary(tune.out.linear)
```

##### Fitting to the full training set

```{r, eval=FALSE}
svm.tuned.linear <- svm(did.exceed ~ ., 
                        data = train1,
                        kernel = "linear",
                        cost = tune.out.linear$best.parameters$cost,
                        probability = TRUE)
```

##### Assessing performance

```{r, eval=FALSE}
# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds.linear <- predict(svm.tuned.linear, newdata = train1, probability = TRUE)

# classification accuracy
mean(train.preds.linear == train1$did.exceed) 

# ROC/AUC
train.roc.linear <- roc(predictions = attr(train.preds.linear, "probabilities")[ , 1],
                        labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc.linear <- auc(train.roc.linear)

# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds.linear <- predict(svm.tuned.linear, newdata = test, probability = TRUE)

# classification accuracy
mean(test.preds.linear == test$did.exceed) 

# ROC/AUC
test.roc.linear <- roc(predictions = attr(test.preds.linear, "probabilities")[ , 1],
                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc.linear <- auc(test.roc.linear)
```

* Classification accuracy on the training set: `r round(mean(train.preds.linear == train1$did.exceed), 3)`
* Classification accuracy on the test set: `r round(mean(test.preds.linear == test$did.exceed), 3)`

```{r training set performance}
plot(train.roc.linear, main = paste("SVM with linear kernel, training metric - AUC:", round(train.auc.linear, 3))) # 0.841
```

```{r test set performance}
plot(test.roc.linear, main = paste("SVM with linear kernel, test metric - AUC:", round(test.auc.linear, 3)))
```

#### Note about ROC/AUC for SVMs

I am a little skeptical about the ROC/AUC for these SVMs. Compared to the test set performance of other models, I am a little hestitant about trusting that the SVM with the linear kernel performed so well on the test set. Classification accuracy for this model was just `r round(mean(test.preds.linear == test$did.exceed)*100, 3)`%, which is actually lower than the accuracy for the SVM with the polynomial kernel, `r round(mean(test.preds == test$did.exceed)*100, 3)`%, that achieved a much lower AUC of `r round(test.auc, 3)`. To look more into this matter, I used the `confusionMatrix()` function from the caret package for more details on model performance - since this function does not require class probabiliites. The following table compares the SVM with a polynomial vs. linear kernel. 

```{r}
svm.linear.cm <- confusionMatrix(data = test.preds.linear,
                                 reference = test$did.exceed)
svm.poly.cm <- confusionMatrix(data = test.preds,
                               reference = test$did.exceed)

# format metrics
knitr::kable(data.frame(Metric = names(svm.linear.cm$byClass),
           'Linear Kernel' = unname(round(svm.linear.cm$byClass, 3)),
           'Polynomial Kernel' = unname(round(svm.poly.cm$byClass, 3))),
           caption = "Metric comparison") %>%
  kableExtra::kable_styling(full_width = F) %>%
  kableExtra::row_spec(c(1, 2, 5, 6, 11), background = "yellow")
```

From the table above, one possible reason for the difference in ROC/AUC scores is that linear kernel's specificity (true negative rate) is much higher than it is for the model with the polynomial kernel. So, the SVM with the linear kernel does a much better job at classifying the days in which PM10 exceeded the standard (the model with the polynomial kernel only predicted 60.5% correctly).

Also, after looking into the `svm` function documentation, I'm still confused about how probabilities are being computed and what exactly happens when `probability = TRUE` is included in the predict function call (`probability = TRUE` vs. `probability = FALSE` lead to slightly different class predictions). Because I'm hestitant about the ROC/AUC for SVMs, I've included output from `confusionMatrix()` for all models to be able to compare other metrics, besides classification accuracy and ROC/AUC, in the Comparing models section.

# Decision tree

```{r, eval=FALSE}
treefit <- tree(did.exceed ~ ., data = train1, split = "gini")
summary(treefit)

# CV to determine optimal size of tree
cv.treefit <- cv.tree(treefit, FUN = prune.misclass)
plot(cv.treefit) # size = 9 did best

pruned.tree <- prune.misclass(treefit, best = 9)
summary(pruned.tree) #  0.09636 = 119 / 1235
plot(pruned.tree)
text(pruned.tree)

# re-train again, using only variables in pruned tree
vars <- unique(as.character(pruned.tree$frame[ , 1]))
vars <- vars[-which(vars == "<leaf>")]
new.train <- train1[ , c("did.exceed", vars)]

new.tree <- tree(did.exceed ~ ., data = new.train, split = "gini")
summary(new.tree) #  0.08259 = 102 / 1235

# predict on test data
tree.preds <- predict(new.tree, newdata = test) %>%
  as.data.frame() %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(tree.preds$actual == tree.preds$prediction) # 0.8924509

# ROC/AUC
roc.metric.tree <- roc(predictions = tree.preds$yes,
                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric.tree <- auc(roc.metric.tree)
```

The decision tree, pruned back to size 7, achieved a test set classification accuracy of `r round(mean(tree.preds$actual == tree.preds$prediction)*100, 3)`%. Test ROC/AUC for this model is:

```{r}
plot(roc.metric.tree, main = paste("Decision tree  -  AUC:", round(auc.metric.tree, 3)))
```

```{r}
# confusion matrix
# precision = 129 / (33 + 129) = 0.796
# out of all that are being classified as high pm10 days - 79.6% are actually high pm10 days
# recall = 129 / (71 + 129) = 0.645 
# out of all days that are actually high pm10, 64.5% are actually being classified as such

table(tree.preds$prediction, tree.preds$actual)
```

# Comparing models

#### Model comparisons based on ROC/AUC

Based on ROC/AUC alone, the support vector machine with a linear kernel and a tuned cost parameter of 0.01 performed the best. 

```{r}
par(mfrow = c(2, 2))
plot(roc.metric, main = paste("Random Forest  -  AUC:", round(auc.metric, 4)))
plot(xgb.roc.metric, main = paste("XGBoost  -  AUC:", round(xgb.auc.metric,4)))
plot(test.roc.linear, main = paste("SVM w/ linear kernel  -  AUC:", round(test.auc.linear, 3)))
plot(roc.metric.tree, main = paste("Decision tree  -  AUC:", round(auc.metric.tree, 4)))
```

#### Model comparisons based on other metrics 

```{r}
rf.cm <- confusionMatrix(rf.preds$prediction, reference = test$did.exceed)
xgb.cm <- confusionMatrix(xgb.preds$prediction, reference = test$did.exceed)
tree.cm <- confusionMatrix(tree.preds$prediction, reference = test$did.exceed)

# format metrics
knitr::kable(data.frame(Metric = names(svm.linear.cm$byClass),
           'Linear Kernel' = unname(round(svm.linear.cm$byClass, 3)),
           'Polynomial Kernel' = unname(round(svm.poly.cm$byClass, 3)),
           'Random forest' = unname(round(rf.cm$byClass, 3)),
           'XGBoost' = unname(round(xgb.cm$byClass, 3)),
           'Decision tree' = unname(round(tree.cm$byClass, 3)))) %>%
  kableExtra::kable_styling(full_width = F) %>%
  kableExtra::row_spec(c(1, 2, 5, 6, 11), background = "yellow")
```

Based on the table above, it also appears that the SVM with the linear kernel performed the best. Below are definitions of some metrics featured above. 

* Sensitivity: True positive rate - if the classifier is able to accurately identify days in which PM10 did not exceed the standard
* Specificity: True negative rate - if the classifier is able to accurately identify days in which PM10 did exceed the standard 
* Precision: Of the days in which it was predicted that PM10 exceeded the standard, this is the percentage of those days that were true (days that PM10 actually did exceed the standard). 
* Recall: Of all days that PM10 actually did exceed the standard, this is the percentage of those days that the classifier predicted correctly (predicted that PM10 exceeded the standard). 
* Balanced accuracy: Good for imbalanced data sets - computed as: `(sensitivity + specificity)/2`

This [link](https://topepo.github.io/caret/measuring-performance.html), under Measures for Predicted Classes, contains more information about each of these metrics. 

