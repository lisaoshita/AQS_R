---
title: "Model Results and Explanations"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# what to include

* objective + basic overview of the goals of this project 
* what packages need to be installed
* what was initially done (classification trees)
* include section for parameter tuning for each model 
* final model comparisons
* variable importance
* take-aways - points for future work

```{r, warning=FALSE, message=FALSE}
# load packages
library(readr)
library(lubridate) # for working with dates
library(dplyr)
library(caret)
library(AUC)
library(openair)
library(tree) # for classification tree at bottom
library(xgboost)
library(e1071) # for SVMs

# load the workspace with tuning/model building results
load("~/Desktop/repos/apcd_r/wind.event.predictor/final.workspace.RData")
```

# Overview

The objective of this project is to build a model to accurately predict wind event days based on meteorological data. For now, a wind event is defined as any day the 24-hour average PM10 concentration at CDF exceeds the state standard, i.e. 50ug/m3. 

#### Models explored

* Random forest
* XGBoosts (extreme Gradient Boosted classification trees)
* Support vector machines (referred to as SVMs)
* regular classification trees

#### General process

For each model, parameter tuning was performed with 5-fold cross-validation on training sets to determine optimal parameter values. Models were then trained on the full training set using those parameters and tested on the held-out test set. Classification accuracy and ROC/AUC metrics were assessed.

Note: all code was commented out to cut down on the time it takes to knit this document. Instead, the workspace was loaded at the beginning of this document to allow for certain objects to be called.

# Setting up the data

This file uses data from S1 and CDF. Models were trained on data from 2011 - 2014 and tested on data from 2015 - 2017. The following code loads the data, partitions the data into training and test sets, and sets up variables. Missing values were assess with a missingness map from the Amelia package, before omitting all rows that contain missing values. 

```{r load data}
# s1.cdf.data <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/forLisa.csv", # read_csv is faster for larger files
#                         col_types = list(date = "c", ws.cdf = "n", wd.cdf = "n",
#                                          pm10.cdf = "n", pm10.oso = "n", wd.s1 = "n",
#                                          ws.s1 = "n", year = "n")) # contains data from 2011 - 2017
# 
# s1.cdf.data <- s1.cdf.data %>%
#   mutate(date = parse_date_time(date, "Ymd HMS"))
# 
# # contains cdf and S1 data up to 2014
# cdf.master <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/cdf.master.csv",
#                        col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n",
#                                         pm10 = "n", u = "n", v = "n", year = "n", 
#                                         precip = "n", s.rad = "n", a.temp = "n",
#                                         rh = "n", dp = "n", s.temp = "n", height = "n",
#                                         temp850 = "n", ws.max = "n", wd.max = "n",
#                                         u.max = "n", v.max = "n", time = "n", dow = "n",
#                                         u.s1 = "n", v.s1 = "n", u.max.s1 = "n", v.max.s1 = "n"))
# 
# cdf.master$date <- date(cdf.master$date)
# 
# # contains cdf and S1 data from 2014 - 2017
# cdf.master2 <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/cdf.master.update.csv",
#                         col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n", pm10 = "n",
#                                         u = "n", v = "n", year = "n", precip = "n",
#                                         s.rad = "n", a.temp = "n", rh = "n", dp = "n",
#                                         s.temp = "n", height = "n", temp850 = "n", ws.max = "n",
#                                         wd.max = "n", u.max = "n", v.max = "n", time = "n",
#                                         dow = "n", u.s1 = "n", v.s1 = "n", u.max.s1 = "n",
#                                         v.max.s1 = "n"))
# 
# cdf.master2$date <- date(cdf.master2$date)

# # ======== SET UP TRAINING DATA ===============================================================
# # train on years before 2015
# training <- s1.cdf.data %>%
#   mutate(year = lubridate::year(date)) %>%
#   mutate(date.only = lubridate::date(date)) %>%
#   filter(year < 2015)


# # finding wd and ws values at CDF and S1 that correspond with high pm10 concentrations
# # using openair package

# ====
# CDF 
# ====
# cdf.training <- training %>%
#   select(date, year, ws.cdf, wd.cdf, pm10.cdf)
# colnames(cdf.training) <- c("date", "year", "ws", "wd", "pm10")
# 
# cdf.clust <- polarCluster(cdf.training, 
#                           pollutant = "pm10",
#                           x = "ws",
#                           wd = "wd",
#                           n.clusters = 2) # cluster 2 is high pm10 
# 
# cdf.clust$data %>%
#   filter(cluster == 2) %>%
#   summarize(min_wd = min(wd),
#             max_wd = max(wd),
#             min_ws = min(ws),
#             max_ws = max(ws)) # CRITERIA: wd between 288 - 320, ws between 9.2 - 20.7
# 
# # ===
# # S1 
# # ===
# s1.training <- training %>%
#   select(date, year, wd.s1, ws.s1, pm10.cdf)
# colnames(s1.training) <- c("date", "year", "wd", "ws", "pm10")
# 
# s1.clust <- polarCluster(s1.training,
#                          pollutant = "pm10",
#                          x = "ws",
#                          wd = "wd",
#                          n.clusters = 2)
# 
# s1.clust$data %>%
#   filter(cluster == 2) %>%
#   summarize(min_wd = min(wd),
#             max_wd = max(wd),
#             min_ws = min(ws),
#             max_ws = max(ws)) # CRITERIA: high pm10 if wd between 281 - 306, ws between 8.88 - 16.11
# 
# # =========================================================================================
# 
# # WD and WS variable set up 
# train1 <- training %>%
#   group_by(date.only) %>%
#   summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE), # maximum WS & WD at CDF and S1
#             max.wd.cdf = max(wd.cdf, na.rm = TRUE),
#             max.ws.s1 = max(ws.s1, na.rm = TRUE),
#             max.wd.s1 = max(wd.s1, na.rm = TRUE),
#             hour.max.wd.s1 = ifelse(length(which.max(wd.s1)) == 0, NA, which.max(wd.s1) - 1), # hour of the day that WS and WD were at maximum
#             hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
#             hour.max.wd.cdf = ifelse(length(which.max(wd.cdf)) == 0, NA, which.max(wd.cdf) - 1),
#             hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
#             ws.in.range.cdf = sum((ws.cdf > 9), na.rm = TRUE), # number of times that WS and WD fell within the range corresponding to high pm10 averages
#             wd.in.range.cdf = sum((wd.cdf > 288) & (wd.cdf < 320), na.rm = TRUE),
#             wd.in.range.s1 = sum((wd.s1 > 281) & (wd.s1 < 306), na.rm = TRUE),
#             ws.in.range.s1 = sum((ws.s1 > 8), na.rm = TRUE)) %>%
#   mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>% # changing "-Inf" values in max variables to NA
#   mutate(max.wd.cdf = ifelse(max.wd.cdf == -Inf, NA, max.wd.cdf)) %>%
#   mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) %>%
#   mutate(max.wd.s1 = ifelse(max.wd.s1 == -Inf, NA, max.wd.s1))
# 
# # =========================================================================================
# 
# # computing pm10 avg 24 hr concentration
# pm10.averages <- training %>%
#   group_by(date.only) %>%
#   summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
#   mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))
# 
# colnames(train1)[1] <- "date"
# 
# # merge train1 with other columns in cdf.master
# train1 <- train1 %>%
#   mutate(did.exceed = pm10.averages$did.exceed) %>% # add did.exceed response variable
#   left_join(cdf.master, by = "date") %>% 
#   mutate(month = month(date)) %>% # create month and day.of.month variables
#   mutate(day.of.month = day(date)) %>%
#   select(did.exceed, ws.in.range.cdf, wd.in.range.cdf, wd.in.range.s1, # retain only a subset of the variables for the model
#          ws.in.range.s1, max.ws.cdf, max.wd.cdf, max.ws.s1, max.wd.s1,
#          hour.max.wd.s1, hour.max.ws.s1, hour.max.wd.cdf, hour.max.wd.cdf, 
#          precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)
# 
# # =========================================================================================
# 
# # examining missing data w/ missingness map 
# # Amelia::missmap(train1) # only 3% of the training data is missing - ok to omit these rows
# train1 <- na.omit(train1) 
# 
# train1$did.exceed <- as.factor(train1$did.exceed)

# # ======== SET UP TEST DATA ===============================================================
# # test on years after 2015
# testing <- s1.cdf.data %>%
#   mutate(year = lubridate::year(date)) %>%
#   mutate(date.only = lubridate::date(date)) %>%
#   filter(year >= 2015)
# 
# # variable set up
# test <- testing %>%
#   group_by(date.only) %>%
#   summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE),
#             max.wd.cdf = max(wd.cdf, na.rm = TRUE),
#             max.ws.s1 = max(ws.s1, na.rm = TRUE),
#             max.wd.s1 = max(wd.s1, na.rm = TRUE),
#             hour.max.wd.s1 = ifelse(length(which.max(wd.s1)) == 0, NA, which.max(wd.s1) - 1),
#             hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
#             hour.max.wd.cdf = ifelse(length(which.max(wd.cdf)) == 0, NA, which.max(wd.cdf) - 1),
#             hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
#             ws.in.range.cdf = sum((ws.cdf > 9), na.rm = TRUE),
#             wd.in.range.cdf = sum((wd.cdf > 288) & (wd.cdf < 320), na.rm = TRUE),
#             wd.in.range.s1 = sum((wd.s1 > 281) & (wd.s1 < 306), na.rm = TRUE),
#             ws.in.range.s1 = sum((ws.s1 > 8), na.rm = TRUE)) %>%
#   mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
#   mutate(max.wd.cdf = ifelse(max.wd.cdf == -Inf, NA, max.wd.cdf)) %>%
#   mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) %>%
#   mutate(max.wd.s1 = ifelse(max.wd.s1 == -Inf, NA, max.wd.s1))
# 
# colnames(test)[1] <- "date"
# 
# # computing 24 hour average pm10 concentration
# pm10.averages.test <- testing %>%
#   group_by(date.only) %>%
#   summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
#   mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))
# 
# # merge test with other columns in cdf.master2
# test <- test %>%
#   mutate(did.exceed = pm10.averages.test$did.exceed) %>%
#   left_join(cdf.master2, by = "date") %>%
#   mutate(month = month(date)) %>%
#   mutate(day.of.month = day(date)) %>%
#   select(did.exceed, ws.in.range.cdf, wd.in.range.cdf, wd.in.range.s1, 
#          ws.in.range.s1, max.ws.cdf, max.wd.cdf, max.ws.s1, max.wd.s1,
#          hour.max.wd.s1, hour.max.ws.s1, hour.max.wd.cdf, hour.max.wd.cdf, 
#          precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)
# 
# # assess rows with missing data
# # Amelia::missmap(test) # only 2% of the training data is missing - ok to omit these rows
# test <- na.omit(test) 
# 
# test$did.exceed <- as.factor(test$did.exceed)
```

# Random forest

#### Parameter tuning

Tuning is performed with functions from the caret package. `tune.control` specifies how to perform the tuning process (in this file: 5-fold cross-validation) and sets other options (e.g. `verboseIter = FALSE` indicates that each iteration should not be printed to the console). `grid` contains all combinations of parameters to assess. The following paramters were tuned: 

* `mtry` is the number of predictors to randomly sample at each split. The default for this parameter is the square root of the number of predictors. 
* `splitrule` determines how splits will be decided. 
* `min.node.size` defines the minimum number of observations within a terminal node. 

```{r}
# parameter tuning set up
# tune.control <- trainControl(method = "cv", 
#                              number = 5,
#                              verboseIter = FALSE,
#                              classProbs = TRUE,
#                              summaryFunction = twoClassSummary) # twoClassSummary: needed for ROC/AUC metrics
#
# grid <- expand.grid(.mtry = c(round(sqrt(ncol(train1))), 
#                               8, 10, 15),
#                     .splitrule = "gini",
#                     .min.node.size = c(5, 10, 20))
```

Tuning is performed with the `train()` function from the caret package. Since the number of trees to grow is not a parameter than can be tuned by including it in the grid, tuning was performed three times to explore different values of `num.trees`. The following are tuning results. 

```{r}
# set.seed(1)
#
# tuning on 500 trees
# rf.tuning.500 <- train(did.exceed ~ ., 
#                        data = train1, 
#                        method = "ranger", # fast implementation of a random forest: ranger, e1071 need to be installed
#                        tuneGrid = grid, 
#                        num.trees = 500,
#                        trControl = tune.control,
#                        importance = "impurity", # allows you to assess variable importance
#                        metric = "ROC")
# rf.tuning.500$bestTune 
knitr::kable(rf.tuning.500$results, caption = "Tuning results with 500 trees") # mtry = 8, min.node.size = 5

# # tuning on 1000 trees
# rf.tuning.1000 <- train(did.exceed ~ ., 
#                         data = train1, 
#                         method = "ranger", 
#                         tuneGrid = grid, 
#                         num.trees = 1000,
#                         trControl = tune.control,
#                         importance = "impurity",
#                         metric = "ROC")
# rf.tuning.1000$bestTune 
knitr::kable(rf.tuning.1000$results, caption = "Tuning results with 1000 trees") # mtry = 5, min.node.size = 5

# # tuning on 1500 trees
# rf.tuning.1500 <- train(did.exceed ~ ., 
#                         data = train1, 
#                         method = "ranger",
#                         tuneGrid = grid, 
#                         num.trees = 1500,
#                         trControl = tune.control,
#                         importance = "impurity",
#                         metric = "ROC")
# rf.tuning.1500$bestTune 
knitr::kable(rf.tuning.1500$results, caption = "Tuning results with 1500 trees") # mtry = 5, min.node.size = 5
```

ROC metrics above are pretty similar, indicating that no model performs drastically better than another. The model that performed the best with an ROC value of 0.953, used 1,500 trees with the following parameter values: 

```{r}
knitr::kable(rf.tuning.1500$bestTune)
plot(rf.tuning.1500)
```

Importance of variables in the model can be assessed with: 

```{r}
plot(varImp(rf.tuning.1000))
```

#### Fitting to the full training set

The following fits the random forest with the optimal paramters found in the previous step, to the full training set and tests it on the held-out set.

```{r}
# ============= Set up parameters: 
# final.grid <- data.frame(.mtry = 5, # contains the optimal parameters 
#                          .splitrule = "gini",
#                          .min.node.size = 5)
# 
# final.control <- trainControl(method = "none", # tells train() to fit the model to the full data without any partitioning
#                               verboseIter = TRUE,
#                               classProbs = TRUE,
#                               summaryFunction = twoClassSummary)

# ============= Fit the model: 
# set.seed(1)
# rf.fit <- train(did.exceed ~ .,
#                 data = train1,
#                 method = "ranger",
#                 tuneGrid = final.grid,
#                 trControl = final.control,
#                 num.trees = 1500,
#                 importance = "impurity",
#                 metric = "ROC")
# 
# ============= Compute predictions: 
# rf.preds <- predict(rf.fit, newdata = test, type = "prob")
# 
# rf.preds <- rf.preds %>%
#   mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
#   mutate(actual = test$did.exceed)
```

The model achieved a classification accuracy of `r round(mean(rf.preds$actual == rf.preds$prediction) * 100, 3)`%.

Because of class imbalance (`r round(sum(train1$did.exceed == "no")/nrow(train1)*100, 3)`% of the days in the training data did not exceed the PM10), metrics that might be better indicators of the model's performance is ROC and AUC. These measures take into account sensitivity (true positive rate) and specificity (true negative rate), rather than just counting the number of correct predictions, which, in our case, is biased. The following computes and displays ROC and AUC for this model, using the AUC package. 

```{r}
# ROC/AUC
# roc.metric <- roc(predictions = rf.preds$yes,
#                   labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
# auc.metric <- auc(roc.metric)

# sensitivity: true positive rate - ability of the model to correctly identify wind event days
# specificity: true negative rate - ability of the model to correctly identify non-wind event days
plot(roc.metric, main = paste("Random Forest  -  AUC:", round(auc.metric, 4)))
```

The following is a confusion matrix of the model's predictions, as well as calculated precision and recall. 

```{r}
table(rf.preds$prediction, rf.preds$actual)
# precision = 140 / (15 + 140) = 0.903 # out of all of the days that are being predicted as wind event days, 90.3% of them actually are wind event days
# recall = 140 / (60 + 140) = 0.70 # out of all of the days that truly were wind event days, 70% of them were classified correctly
```





# XGBoosts

#### Parameter tuning 

XGBoost algorithms are a fairly new method of supervised learning that tend to perform consistently better than other models. It is a form of gradient boosting that introduces a different, more formal method of regularization to prevent overfitting---enabling it to outperform other models. Additionally, XGBoost algorithms are parallelizable, allowing it to fully utilize the power of computers, which effectively decreases computation time.

The following tunes some of the XGBoost parameters. This [link](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster) defines each of the parameters. 

```{r}
# # parameter tuning
# xgb.tune.grid <- expand.grid(nrounds = c(500, 1000),
#                              max_depth = c(3, 6, 10),
#                              eta = 0.3,
#                              gamma = 1,
#                              min_child_weight = 1,
#                              colsample_bytree = c(0.5, 0.8),
#                              subsample = c(0.5, 0.8))
# 
# tune.control <- trainControl(method = "cv",
#                              number = 5,
#                              verboseIter = TRUE,
#                              classProbs = TRUE,
#                              summaryFunction = twoClassSummary)
# 
# xgb.tune <- train(did.exceed ~ .,
#                   data = train1,
#                   method = "xgbTree",
#                   tuneGrid = xgb.tune.grid,
#                   trControl = tune.control,
#                   importance = "impurity",
#                   metric = "ROC")
```

The following display tuning results and variable importance. Parameters that lead to the best performance (with an ROC of 0.943) were: 

```{r}
knitr::kable(xgb.tune$bestTune)

knitr::kable(xgb.tune$results, caption = "XGBoost tuning results")

plot(xgb.tune)

plot(varImp(xgb.tune))
```

#### Fitting to the full training set

```{r}
# =======================================================================
# # fit to full training set
# xgb.final <- train(did.exceed ~ .,
#                    data = train1,
#                    method = "xgbTree",
#                    tuneGrid = xgb.tune$bestTune,
#                    trControl = trainControl(method = "none",
#                                             verboseIter = TRUE,
#                                             classProbs = TRUE,
#                                             summaryFunction = twoClassSummary),
#                    importance = "impurity",
#                    metric = "ROC")


# =======================================================================
# # assess performance
# xgb.preds <- predict(xgb.final, newdata = test, type = "prob") %>%
#   as.data.frame() %>%
#   mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
#   mutate(actual = test$did.exceed)

# # classification accuracy
# mean(xgb.preds$actual == xgb.preds$prediction)

# # ROC/AUC
# xgb.roc.metric <- roc(predictions = xgb.preds$yes,
#                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
# xgb.auc.metric <- auc(xgb.roc.metric)
```

The XGBoost fit to the full training set and tested on the held-out set achieved a classification accuracy of `r round(mean(xgb.preds$actual == xgb.preds$prediction)*100, 3)`%, and an ROC/AUC of: 

```{r}
plot(xgb.roc.metric, main = paste("XGBoost  -  AUC:", round(xgb.auc.metric,4)))
```

Confusion matrix of the model's predictions: 

```{r}
table(xgb.preds$prediction, xgb.preds$actual)
```

In the xgboost.Rmd file, I performed another round of parameter tuning with higher values of `nrounds` and a smaller value of `eta`. `eta` controls the learning rate (models that learn slower tend to perform well). A slower learning rate must be countered with higher values of `nrounds`. However, results from these models did not differ from the results above so they are not discussed here. 

# Support vector machine

Support vector machines work by using hyperplanes to partition the data. So they work well when the data is separable. Since we saw that days with high PM10 concentrations could be clustered according to wind direction and wind speed (using the openair package), I thought SVMs with a polynomial kernel might perform well. (A polynomial kernel determines the way that the hyperplane separates the data - instead of using a linear, straight hyperplane, a polynomial kernel allows for that hyperplane to be curved - other options are radial kernels).

#### Parameter tuning

Parameter tuning was performed with the `tune()` function from the e1071 package (tuning with the `train()` function from the caret package consistently returned errors). [This](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93) is a good source for more information on the cost parameter tuned here. 

```{r}
# # tuning svm with polynomial kernel
# tune.out <- tune(svm, did.exceed ~ ., 
#                  data = train1, 
#                  kernel = "polynomial",
#                  degree = 2,
#                  ranges = list(cost = c(.01, .1, .5, 1, 5, 7, 10)))

summary(tune.out)
```



```{r}


# # svm with tuned parameter on full training set
# svm.tuned <- svm(did.exceed ~ ., 
#                  data = train1,
#                  kernel = "polynomial",
#                  degree = 2,
#                  cost = 5)

# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds <- predict(svm.tuned, newdata = train1)

# classification accuracy
mean(train.preds == train1$did.exceed) # 0.9238866

# ROC/AUC 
train.roc <- roc(predictions = train.preds,
                 labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc <- auc(train.roc)
plot(train.roc, main = paste("AUC:", train.auc))

# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds <- predict(svm.tuned, newdata = test)

# classification accuracy
mean(test.preds == test$did.exceed) # 0.8945191

# ROC/AUC
test.roc <- roc(predictions = test.preds,
                labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc <- auc(test.roc)
plot(test.roc, main = paste("AUC:", test.auc))
```





