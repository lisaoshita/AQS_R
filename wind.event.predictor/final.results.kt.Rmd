---
title: "bettercluster"
author: "Karl Tupper"
date: "November 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
# load packages
library(readr)
library(lubridate) # for working with dates
library(dplyr)
library(caret)
library(AUC)
library(openair)
library(tree) # for decision tree at bottom
library(xgboost)
library(e1071) # for SVMs

# load the workspace with tuning/model building results
# load("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/final.workspace.RData")
```

# Overview

The objective of this project is to build a model to accurately predict wind event days based on meteorological data. For now, a wind event is defined as any day the 24-hour average PM10 concentration at CDF exceeds the state standard, i.e. 50ug/m3. 

#### Models explored

* Random forest
* XGBoosts (extreme Gradient Boosted classification trees)
* Support vector machines (referred to as SVMs)
* regular decision trees

#### General process

For each model, parameter tuning was performed with 5-fold cross-validation on training sets to determine optimal parameter values. Models were then trained on the full training set using those parameters and tested on the held-out test set. Classification accuracy and ROC/AUC metrics were assessed.

Note: all code was commented out to cut down on the time it takes to knit this document. Instead, the workspace was loaded at the beginning of this document to allow for certain objects to be called.

# Setting up the data

This file uses data from S1 and CDF. Models were trained on data from 2011 - 2014 and tested on data from 2015 - 2017. The following code loads the data, partitions the data into training and test sets, and sets up variables. Before omitting all rows that contain missing values, the pattern of missing values was assessed with a missingness map from the Amelia package.

```{r load data}
# # read_csv is faster for larger files
s1.cdf.data <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/forLisa.csv", 
                        col_types = list(date = "c", ws.cdf = "n", wd.cdf = "n",
                                         pm10.cdf = "n", pm10.oso = "n", wd.s1 = "n",
                                         ws.s1 = "n", year = "n")) # contains data from 2011 - 2017

s1.cdf.data <- s1.cdf.data %>%
  mutate(date = parse_date_time(date, "Ymd HMS"))

# contains cdf and S1 data up to 2014
cdf.master <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/cdf.master.csv",
                       col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n",
                                        pm10 = "n", u = "n", v = "n", year = "n", 
                                        precip = "n", s.rad = "n", a.temp = "n",
                                        rh = "n", dp = "n", s.temp = "n", height = "n",
                                        temp850 = "n", ws.max = "n", wd.max = "n",
                                        u.max = "n", v.max = "n", time = "n", dow = "n",
                                        u.s1 = "n", v.s1 = "n", u.max.s1 = "n", v.max.s1 = "n"))
 
cdf.master$date <- date(cdf.master$date)

# contains cdf and S1 data from 2014 - 2017
cdf.master2 <- read_csv("H:/TECH/Lisa/R/apcd.r/wind.event.predictor/cdf.master.update.csv",
                        col_types = list(date = "c", ws = "n", wd = "n", pm25 = "n", pm10 = "n",
                                        u = "n", v = "n", year = "n", precip = "n",
                                        s.rad = "n", a.temp = "n", rh = "n", dp = "n",
                                        s.temp = "n", height = "n", temp850 = "n", ws.max = "n",
                                        wd.max = "n", u.max = "n", v.max = "n", time = "n",
                                        dow = "n", u.s1 = "n", v.s1 = "n", u.max.s1 = "n",
                                        v.max.s1 = "n"))

cdf.master2$date <- date(cdf.master2$date)

# ======== SET UP TRAINING DATA ===============================================================
# train on years before 2015
training <- s1.cdf.data %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(date.only = lubridate::date(date)) %>%
  filter(year < 2015)


# finding wd and ws values at CDF and S1 that correspond with high pm10 concentrations
# using openair package

# ====
# CDF 
# ====
cdf.training <- training %>%
  select(date, year, ws.cdf, wd.cdf, pm10.cdf)
colnames(cdf.training) <- c("date", "year", "ws", "wd", "pm10")

cdf.clust <- polarCluster(cdf.training, 
                          pollutant = "pm10",
                          x = "ws",
                          wd = "wd",
                          n.clusters = 2) # cluster 2 is high pm10 

cdf.clust$data %>%
  filter(cluster == 2) %>%
  summarize(min_wd = min(wd),
            max_wd = max(wd),
            min_ws = min(ws),
            max_ws = max(ws)) # CRITERIA: wd between 288 - 320, ws between 9.2 - 20.7

# ===
# S1 
# ===
s1.training <- training %>%
  select(date, year, wd.s1, ws.s1, pm10.cdf)
colnames(s1.training) <- c("date", "year", "wd", "ws", "pm10")

s1.clust <- polarCluster(s1.training,
                         pollutant = "pm10",
                         x = "ws",
                         wd = "wd",
                         n.clusters = 2)

s1.clust$data %>%
  filter(cluster == 2) %>%
  summarize(min_wd = min(wd),
            max_wd = max(wd),
            min_ws = min(ws),
            max_ws = max(ws)) # CRITERIA: high pm10 if wd between 281 - 306, ws between 8.88 - 16.11
# ===================================================================
# figure out clusters better

library(sp)

# functions to convert polar coordinates to cartesian:
make.x <- function(ws, wd){
  ws*cos((90-wd)*pi/180)
}

make.y <- function(ws, wd){
  ws*sin((90-wd)*pi/180)
}

# get cluster of high PM and create cartersian coordinates:
s1.clust$data %>% 
  filter(cluster == 2) %>%
  mutate(x = make.x(ws, wd)) %>%
  mutate(y = make.y(ws, wd)) -> s1.range

# check (looks reasoable):
summary(s1.range)       
plot(s1.range$x, s1.range$y,  #compare with print(s1.clust) 
     ylim = c(-15, 15), xlim = c(-15, 15),
     asp = 1, pch = 16, cex = 0.5)

# get convex hull
chull.index <- chull(s1.range$x, s1.range$y)
chull.index <- c(chull.index, chull.index[1])
s1.range.chull <- s1.range[chull.index, c("x", "y")]

# check --> looks good!
lines(s1.range.chull$x, s1.range.chull$y,
      col = "red") # add to current plot

## alternative: (actually don't need to convert to cartesian coords)
#chull.index <- chull(s1.range$wd, s1.range$ws)
#chull.index <- c(chull.index, chull.index[1])
#s1.range.chull <- s1.range[chull.index, c("x", "y")]
#lines(s1.range.chull$x, s1.range.chull$y,
#      col = "green") # add to current plot


# function to see if a point falls in the s1 range:
wind.in.range <- function(ws, wd, range){
  # assumes range is a two column df with "x" and "y"
  
  # assumes ws and wd in usual format, 
  # so must convert to cartesian coords.
  # define these functions again, in case they are not 
  # in environment:
  
  make.x <- function(ws, wd){
    ws*cos((90-wd)*pi/180)
  }
  
  make.y <- function(ws, wd){
    ws*sin((90-wd)*pi/180)
  }
  
  xs <- make.x(ws, wd)
  ys <- make.y(ws, wd)
  
  # test if in range
  res <- sp::point.in.polygon(xs, ys, range$x, range$y)
  
  # return 0 if outside, 1 if inside or on edge, NA if ws or wd is missing
  res <- ifelse(res == 0, 0, 1) # see ?point.in.polygon
  res[is.na(ws) | is.na(wd)] <- NA # preserve NA's
  return(res) 
}

## test function: does it 'predict' if ws/wd pair is in high PM10 cluster?

preds <- wind.in.range(s1.clust$data$ws, s1.clust$data$wd, s1.range.chull)
table(s1.clust$data$cluster, preds)

# hmmm... 52 observation from cluster 1 are predicted to be in cluster 2
s1.clust$data[preds == 1 & s1.clust$data$cluster == 1, ]
# these all appear to be on the edge, so OK if predicted to be in cluster 2

# same for CDF:
# get cluster of high PM and create cartersian coordinates:
cdf.clust$data %>% 
  filter(cluster == 2) %>%
  mutate(x = make.x(ws, wd)) %>%
  mutate(y = make.y(ws, wd)) -> cdf.range

# check (looks reasoable):
summary(cdf.range)       
plot(cdf.range$x, cdf.range$y,  #compare with print(cdf.clust) 
     ylim = c(-12, 12), xlim = c(-12, 12),
     asp = 1, pch = 16, cex = 0.5)

# get convex hull
chull.index <- chull(cdf.range$x, cdf.range$y)
chull.index <- c(chull.index, chull.index[1])
cdf.range.chull <- cdf.range[chull.index, c("x", "y")]

# check --> looks good!
lines(cdf.range.chull$x, cdf.range.chull$y,
      col = "red") # add to current plot
preds <- wind.in.range(cdf.clust$data$ws, cdf.clust$data$wd,cdf.range.chull)
table(cdf.clust$data$cluster, preds)


# =========================================================================================
# 
# WD and WS variable set up 
# - maximum WS & WD at CDF and S1
# - hour of the day that WS and WD were at maximum
# - number of times that WS and WD fell within the range corresponding to high pm10 averages

train1 <- training %>%
  group_by(date.only) %>%
  summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE), 
            #max.wd.cdf = max(wd.cdf, na.rm = TRUE),
            max.ws.s1 = max(ws.s1, na.rm = TRUE),
            #max.wd.s1 = max(wd.s1, na.rm = TRUE),
            #hour.max.wd.s1 = ifelse(length(which.max(wd.s1)) == 0, NA, which.max(wd.s1) - 1), 
            hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
            #hour.max.wd.cdf = ifelse(length(which.max(wd.cdf)) == 0, NA, which.max(wd.cdf) - 1),
            hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
            wind.in.range.cdf = sum(wind.in.range(ws.cdf, wd.cdf, cdf.range.chull), na.rm = TRUE), 
            #wd.in.range.cdf = sum((wd.cdf > 288) & (wd.cdf < 320), na.rm = TRUE),
            #wd.in.range.s1 = sum((wd.s1 > 281) & (wd.s1 < 306), na.rm = TRUE),
            wind.in.range.s1 = sum(wind.in.range(ws.s1, wd.s1, s1.range.chull), na.rm = TRUE)) %>%
  mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
  #mutate(max.wd.cdf = ifelse(max.wd.cdf == -Inf, NA, max.wd.cdf)) %>%
  mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) # %>%
  #mutate(max.wd.s1 = ifelse(max.wd.s1 == -Inf, NA, max.wd.s1))

# =========================================================================================
# 
# # computing pm10 avg 24 hr concentration
pm10.averages <- training %>%
  group_by(date.only) %>%
  summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
  mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))

colnames(train1)[1] <- "date"

# merge train1 with other columns in cdf.master
train1 <- train1 %>%
  mutate(did.exceed = pm10.averages$did.exceed) %>% # add did.exceed response variable
  left_join(cdf.master, by = "date") %>% 
  mutate(month = month(date)) %>% # create month and day.of.month variables
  mutate(day.of.month = day(date)) %>%
  select(did.exceed, wind.in.range.cdf, wind.in.range.s1,
         max.ws.cdf, max.ws.s1,
         hour.max.ws.s1, hour.max.ws.cdf, 
         precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)

# # =========================================================================================
# 
# # examining missing data w/ missingness map 
# # Amelia::missmap(train1) # only 3% of the training data is missing - ok to omit these rows
train1 <- na.omit(train1) 

train1$did.exceed <- as.factor(train1$did.exceed)

# ======== SET UP TEST DATA ===============================================================
# test on years after 2015
testing <- s1.cdf.data %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(date.only = lubridate::date(date)) %>%
  filter(year >= 2015)

# variable set up
test <- testing %>%
  group_by(date.only) %>%
  summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE),
            max.wd.cdf = max(wd.cdf, na.rm = TRUE),
            max.ws.s1 = max(ws.s1, na.rm = TRUE),
            max.wd.s1 = max(wd.s1, na.rm = TRUE),
            hour.max.wd.s1 = ifelse(length(which.max(wd.s1)) == 0, NA, which.max(wd.s1) - 1),
            hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
            hour.max.wd.cdf = ifelse(length(which.max(wd.cdf)) == 0, NA, which.max(wd.cdf) - 1),
            hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
            ws.in.range.cdf = sum((ws.cdf > 9), na.rm = TRUE),
            wd.in.range.cdf = sum((wd.cdf > 288) & (wd.cdf < 320), na.rm = TRUE),
            wd.in.range.s1 = sum((wd.s1 > 281) & (wd.s1 < 306), na.rm = TRUE),
            ws.in.range.s1 = sum((ws.s1 > 8), na.rm = TRUE)) %>%
  mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
  mutate(max.wd.cdf = ifelse(max.wd.cdf == -Inf, NA, max.wd.cdf)) %>%
  mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) %>%
  mutate(max.wd.s1 = ifelse(max.wd.s1 == -Inf, NA, max.wd.s1))
 
colnames(test)[1] <- "date"

# computing 24 hour average pm10 concentration
pm10.averages.test <- testing %>%
  group_by(date.only) %>%
  summarize(pm10.ave = mean(pm10.cdf, na.rm = TRUE)) %>%
  mutate(did.exceed = ifelse(pm10.ave >= 50, "yes", "no"))

# merge test with other columns in cdf.master2
test <- test %>%
  mutate(did.exceed = pm10.averages.test$did.exceed) %>%
  left_join(cdf.master2, by = "date") %>%
  mutate(month = month(date)) %>%
  mutate(day.of.month = day(date)) %>%
  select(did.exceed, ws.in.range.cdf, wd.in.range.cdf, wd.in.range.s1, 
         ws.in.range.s1, max.ws.cdf, max.wd.cdf, max.ws.s1, max.wd.s1,
         hour.max.wd.s1, hour.max.ws.s1, hour.max.wd.cdf, hour.max.wd.cdf, 
         precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)

# assess rows with missing data
# Amelia::missmap(test) # only 2% of the training data is missing - ok to omit these rows
test <- na.omit(test) 

test$did.exceed <- as.factor(test$did.exceed)
```

# Random forest

#### Parameter tuning

Tuning is performed with functions from the caret package. `tune.control` specifies how to perform the tuning process (in this file: 5-fold cross-validation) and sets other options (e.g. `verboseIter = FALSE` indicates that each iteration should not be printed to the console). `grid` contains all combinations of parameters to assess. The following parameters were tuned: 

* `mtry` is the number of predictors to randomly sample at each split. The default for this parameter is the square root of the number of predictors. 
* `splitrule` determines how splits will be decided. 
* `min.node.size` defines the minimum number of observations within a terminal node. 

```{r}
# parameter tuning set up
tune.control <- trainControl(method = "cv", 
                             number = 5,
                             verboseIter = FALSE,
                             classProbs = TRUE,
                             summaryFunction = twoClassSummary) # twoClassSummary: needed for ROC/AUC metrics
#
grid <- expand.grid(.mtry = c(round(sqrt(ncol(train1))), 
                             8, 10, 15),
                    .splitrule = "gini",
                    .min.node.size = c(5, 10, 20))
```

Tuning is performed with the `train()` function from the caret package. Since the number of trees to grow is not a parameter than can be tuned by including it in the grid, tuning was performed three times to explore different values of `num.trees`. The following are tuning results. 

More details on parameter tuning with the train function can be found [here](http://topepo.github.io/caret/train-models-by-tag.html).

```{r}
set.seed(1)

# tuning on 500 trees
rf.tuning.500 <- train(did.exceed ~ ., 
                      data = train1, 
                      method = "ranger", # fast implementation of a random forest: ranger, e1071 need to be installed
                       tuneGrid = grid, 
                       num.trees = 500,
                       trControl = tune.control,
                       importance = "impurity", # allows you to assess variable importance
                       metric = "ROC")
rf.tuning.500$bestTune 
knitr::kable(rf.tuning.500$results, caption = "Tuning results with 500 trees") # mtry = 8, min.node.size = 5

# tuning on 1000 trees
rf.tuning.1000 <- train(did.exceed ~ ., 
                        data = train1, 
                        method = "ranger", 
                        tuneGrid = grid, 
                        num.trees = 1000,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")
rf.tuning.1000$bestTune 
knitr::kable(rf.tuning.1000$results, caption = "Tuning results with 1000 trees") # mtry = 5, min.node.size = 5

# tuning on 1500 trees
rf.tuning.1500 <- train(did.exceed ~ .,
                        data = train1,
                        method = "ranger",
                        tuneGrid = grid,
                        num.trees = 1500,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")
rf.tuning.1500$bestTune
knitr::kable(rf.tuning.1500$results, caption = "Tuning results with 1500 trees") # mtry = 5, min.node.size = 5
```

ROC metrics above are pretty similar, indicating that no model performs drastically better than another. The model that performed the best with an ROC value of 0.953, used 1,500 trees with the following parameter values: 

```{r}
knitr::kable(rf.tuning.1500$bestTune)
plot(rf.tuning.1500)
```

Importance of variables in the model can be assessed with: 

```{r}
plot(varImp(rf.tuning.1500))
```

#### Fitting to the full training set

The following fits the random forest with the optimal parameters found in the previous step, to the full training set and tests it on the held-out set.

```{r}
# ============= Set up parameters:
final.grid <- data.frame(.mtry = 4, # contains the optimal parameters
                         .splitrule = "gini",
                         .min.node.size = 5)

final.control <- trainControl(method = "none", # tells train() to fit the model to the full data without any partitioning
                              verboseIter = TRUE,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

# ============= Fit the model:
set.seed(1)
rf.fit <- train(did.exceed ~ .,
                data = train1,
                method = "ranger",
                tuneGrid = final.grid,
                trControl = final.control,
                num.trees = 1500,
                importance = "impurity",
                metric = "ROC")
```

#### Assessing performance

```{r}
# ============= Compute predictions: 
rf.preds <- predict(rf.fit, newdata = test, type = "prob")

rf.preds <- rf.preds %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)
```

The model achieved a classification accuracy of `r round(mean(rf.preds$actual == rf.preds$prediction) * 100, 3)`%.

Because of class imbalance (`r round(sum(train1$did.exceed == "no")/nrow(train1)*100, 3)`% of the days in the training data did not exceed the PM10), metrics that might be better indicators of the model's performance is ROC and AUC. These measures take into account sensitivity (true positive rate) and specificity (true negative rate), rather than just counting the number of correct predictions, which, in our case, is biased. The following computes and displays ROC and AUC for this model, using the AUC package. 

```{r}
# ROC/AUC
roc.metric <- roc(predictions = rf.preds$yes,
                  labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric <- auc(roc.metric)

# sensitivity: true positive rate - ability of the model to correctly identify wind event days
# specificity: true negative rate - ability of the model to correctly identify non-wind event days
plot(roc.metric, main = paste("Random Forest  -  AUC:", round(auc.metric, 4)))
```

The following is a confusion matrix of the model's predictions, as well as calculated precision and recall. 

```{r}
# precision = 140 / (15 + 140) = 0.903 # out of all of the days that are being predicted as wind event days, 90.3% of them actually are wind event days
# recall = 140 / (60 + 140) = 0.70 # out of all of the days that truly were wind event days, 70% of them were classified correctly
table(rf.preds$prediction, rf.preds$actual)
```

# XGBoosts

#### Parameter tuning 

XGBoost algorithms are a fairly new method of supervised learning that tend to perform consistently better than other models. It is a form of gradient boosting that introduces a different, more formal method of regularization to prevent overfitting---enabling it to outperform other models. Additionally, XGBoost algorithms are parallelizable, allowing it to fully utilize the power of computers, which effectively decreases computation time.

The following tunes some of the XGBoost parameters. This [link](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster) provides details on each of the parameters.

```{r}
# parameter tuning
xgb.tune.grid <- expand.grid(nrounds = c(500, 1000),
                             max_depth = c(3, 6, 10),
                             eta = 0.3,
                             gamma = 1,
                             min_child_weight = 1,
                             colsample_bytree = c(0.5, 0.8),
                             subsample = c(0.5, 0.8))

tune.control <- trainControl(method = "cv",
                             number = 5,
                             verboseIter = TRUE,
                             classProbs = TRUE,
                             summaryFunction = twoClassSummary)

xgb.tune <- train(did.exceed ~ .,
                  data = train1,
                  method = "xgbTree",
                  tuneGrid = xgb.tune.grid,
                  trControl = tune.control,
                  importance = "impurity",
                  metric = "ROC")
```

The following displays tuning results and variable importance. Parameters that lead to the best performance (with an ROC of 0.943) were: 

```{r}
knitr::kable(xgb.tune$bestTune)

knitr::kable(xgb.tune$results, caption = "XGBoost tuning results")

plot(xgb.tune)

plot(varImp(xgb.tune))
```

#### Fitting to the full training set

```{r}
# fit to full training set
xgb.final <- train(did.exceed ~ .,
                   data = train1,
                   method = "xgbTree",
                   tuneGrid = xgb.tune$bestTune,
                   trControl = trainControl(method = "none",
                                            verboseIter = TRUE,
                                            classProbs = TRUE,
                                            summaryFunction = twoClassSummary),
                   importance = "impurity",
                   metric = "ROC")
```

#### Assessing performance

```{r}
# assess performance
xgb.preds <- predict(xgb.final, newdata = test, type = "prob") %>%
  as.data.frame() %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(xgb.preds$actual == xgb.preds$prediction)

# ROC/AUC
xgb.roc.metric <- roc(predictions = xgb.preds$yes,
                      labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
xgb.auc.metric <- auc(xgb.roc.metric)
```

The XGBoost fit to the full training set and tested on the held-out set achieved a classification accuracy of `r round(mean(xgb.preds$actual == xgb.preds$prediction)*100, 3)`%, and an ROC/AUC of: 

```{r}
plot(xgb.roc.metric, main = paste("XGBoost  -  AUC:", round(xgb.auc.metric,4)))
```

Confusion matrix of the model's predictions: 

```{r}
table(xgb.preds$prediction, xgb.preds$actual)
```

In the xgboost.Rmd file, I performed another round of parameter tuning with higher values of `nrounds` and a smaller value of `eta`. `eta` controls the learning rate (models that learn slower tend to perform well). A slower learning rate must be countered with higher values of `nrounds`. However, results from these models did not differ from the results above so they are not discussed here. 

# Support vector machines

Support vector machines use hyperplanes to partition the data. So they work well when the data is separable. Since we saw that days with high PM10 concentrations could be clustered according to wind direction and wind speed (using the openair package), I thought SVMs with a polynomial kernel might perform well. [This](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93) is a good source for more information on SVMs and the cost parameter tuned here. 

#### Parameter tuning

Parameter tuning was performed with the `tune()` function from the e1071 package (tuning with the `train()` function from the caret package consistently returned errors). 

```{r}
# tuning svm with polynomial kernel
tune.out <- tune(svm, did.exceed ~ .,
                 data = train1,
                 kernel = "polynomial",
                 degree = 2,
                 ranges = list(cost = c(.01, .1, .5, 1, 5, 7, 10)))
summary(tune.out)
```

The SVM that performed the best, with an error rate of 0.104 (classification accuracy of 89.6%), used a cost parameter of 5. 

#### Fitting to the full training set

```{r}
# svm with tuned parameter on full training set
svm.tuned <- svm(did.exceed ~ .,
                 data = train1,
                 kernel = "polynomial",
                 degree = 2,
                 cost = 5) # tuned cost parameter
```

#### Assessing performance

```{r}
# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds <- predict(svm.tuned, newdata = train1)

# classification accuracy
mean(train.preds == train1$did.exceed) # 0.9238866

# ROC/AUC
train.roc <- roc(predictions = train.preds,
                 labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc <- auc(train.roc)
```

The SVM achieved a training classification accuracy of `r round(mean(train.preds == train1$did.exceed)*100, 3)`%. Training ROC/AUC is: 

```{r}
plot(train.roc, main = paste("SVM, training metric  -  AUC:", round(train.auc, 3)))
```

```{r}
# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds <- predict(svm.tuned, newdata = test)

# classification accuracy
mean(test.preds == test$did.exceed) # 0.8945191

# ROC/AUC
test.roc <- roc(predictions = test.preds,
                labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc <- auc(test.roc)
```

Tested on the held-out set, the SVM achieved a classification accuracy of `r round(mean(test.preds == test$did.exceed)*100, 3)`%. ROC/AUC on the test set was: 

```{r}
plot(test.roc, main = paste("SVM, test metric  -  AUC:", round(test.auc, 3)))
```

There is some evidence that this SVM was overfit to the training data - as indicated by the model's poor performance on the test set. So, in the svm.Rmd file, I explored the possibility of an SVM with a linear kernel, instead of a polynomial kernel, but achieved similar results. 

# Decision tree

The following fits a decision tree to the training data following a similar process used for the first decision tree [here](https://github.com/sloapcdkt/2016aqrptR/blob/master/appendix.R). 

```{r}
treefit <- tree(did.exceed ~ ., data = train1, split = "gini" )
summary(treefit)

# CV to determine optimal size of tree
cv.treefit <- cv.tree(treefit, FUN = prune.misclass)
cv.treefit
plot(cv.treefit) # size = 6 or 7 did best

pruned.tree <- prune.misclass(treefit, best = 7)
summary(pruned.tree) #  0.09636 = 119 / 1235
plot(pruned.tree)
text(pruned.tree)

# re-train again, using only variables in pruned tree
vars <- unique(as.character(pruned.tree$frame[ , 1]))
vars <- vars[-which(vars == "<leaf>")]
new.train <- train1[ , c("did.exceed", vars)]

new.tree <- tree(did.exceed ~ ., data = new.train, split = "gini")
summary(new.tree) #  0.08259 = 102 / 1235

# predict on test data
tree.preds <- predict(new.tree, newdata = test) %>%
  as.data.frame() %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(tree.preds$actual == tree.preds$prediction)

# ROC/AUC
roc.metric.tree <- roc(predictions = tree.preds$yes,
                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric.tree <- auc(roc.metric.tree)
```

The decision tree, pruned back to size 7, achieved a test set classification accuracy of `r round(mean(tree.preds$actual == tree.preds$prediction)*100, 3)`%. Test ROC/AUC for this model is:

```{r}
plot(roc.metric.tree, main = paste("Decision tree  -  AUC:", round(auc.metric.tree, 3)))
```

```{r}
# confusion matrix
# precision = 127 / (34 + 127) = 0.789 (out of all that are being classified as high pm10 days - 78.9% are actually high pm10 days)
# recall = 127 / (73 + 127) = 0.635 (out of all days that are actually high pm10, 63.5% are actually being classified as such) 
table(tree.preds$prediction, tree.preds$actual)
```

# Comparing models

Based on ROC/AUC alone, the random forest performed the best. 

```{r}
par(mfrow = c(2, 2))
plot(roc.metric, main = paste("Random Forest  -  AUC:", round(auc.metric, 4)))
plot(xgb.roc.metric, main = paste("XGBoost  -  AUC:", round(xgb.auc.metric,4)))
plot(test.roc, main = paste("SVM, test metric  -  AUC:", round(test.auc, 4)))
plot(roc.metric.tree, main = paste("Decision tree  -  AUC:", round(auc.metric.tree, 4)))
```

# Notes

Since performance for every model was mostly similar (with the exception of the SVM) and further tuning didn't seem to lead to much improvement, I think a next possible step is to think of more variables to include. For both the random forest and XGBoost algorithms, the variables that were the most important regarded wind direction and wind speed at S1. So, as a point for further work, I think including more information/variables from S1 may help with model accuracy.

```{r}
plot(varImp(rf.tuning.1500))
```

