---
title: "Wind Event Predictor - Support Vector Machine"
date: "October 24, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tuning SVM with polynomial kernel

```{r svm tuning}
# tuning svm with polynomial kernel
tune.out <- tune(svm, did.exceed ~ ., 
                 data = train1, 
                 kernel = "polynomial",
                 degree = 2,
                 ranges = list(cost = c(.01, .1, .5, 1, 5, 7, 10)),
                 probability = TRUE)

summary(tune.out)

# svm with tuned parameter on full training set
svm.tuned <- svm(did.exceed ~ ., 
                 data = train1,
                 kernel = "polynomial",
                 degree = 2,
                 cost = tune.out$best.parameters$cost,
                 probability = TRUE)
str(svm.tuned)
# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds <- predict(svm.tuned, newdata = train1, probability = TRUE)

# classification accuracy
mean(train.preds == train1$did.exceed) # 0.9279352

# ROC/AUC 
train.roc <- roc(predictions = attr(train.preds, "probabilities")[ , 1],
                 labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc <- auc(train.roc)
plot(train.roc, main = paste("AUC:", train.auc))

# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds <- predict(svm.tuned, newdata = test, probability = TRUE)

# classification accuracy
mean(test.preds == test$did.exceed) # 0.8862461

# ROC/AUC
test.roc <- roc(predictions = attr(test.preds, "probabilities")[ , 1],
                labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc <- auc(test.roc)
plot(test.roc, main = paste("AUC:", test.auc))
```

# Tuning SVM with linear kernel

```{r}
# tuning svm with linear kernel
tune.out.linear <- tune(svm, did.exceed ~ ., 
                        data = train1,
                        kernel = "linear",
                        ranges = list(cost = c(0.001, .01, .1, .5, 1, 5, 7, 10)),
                        probability = TRUE)

summary(tune.out.linear) # best with C = 0.1

svm.tuned.linear <- svm(did.exceed ~ ., 
                        data = train1,
                        kernel = "linear",
                        cost = tune.out.linear$best.parameters$cost,
                        probability = TRUE)

# ------------------------------------------
# assessing training error + ROC/AUC metric
# ------------------------------------------
train.preds.linear <- predict(svm.tuned.linear, newdata = train1, probability = TRUE)

# classification accuracy
mean(train.preds.linear == train1$did.exceed) # 0.9174089

# ROC/AUC
train.roc.linear <- roc(predictions = attr(train.preds.linear, "probabilities")[ , 1],
                        labels = as.factor(ifelse(as.character(train1$did.exceed) == "yes", 1, 0)))
train.auc.linear <- auc(train.roc.linear)
plot(train.roc.linear, main = paste("AUC:", train.auc.linear)) # .943

# --------------------------------------
# assessing test error + ROC/AUC metric
# --------------------------------------
test.preds.linear <- predict(svm.tuned.linear, newdata = test, probability = TRUE)

# classification accuracy
mean(test.preds.linear == test$did.exceed) # 0.885212

# ROC/AUC
test.roc.linear <- roc(predictions = attr(test.preds.linear, "probabilities")[ , 1],
                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
test.auc.linear <- auc(test.roc.linear)
plot(test.roc.linear, main = paste("AUC:", test.auc.linear))
```
