---
title: "Wind Event Predictor - Random forest"
date: "October 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Random Forest parameter tuning

```{r}
grid <- expand.grid(.mtry = c(round(sqrt(ncol(train1))), 
                              8, 10, 15),
                    .splitrule = "gini",
                    .min.node.size = c(5, 10, 20))

tune.control <- trainControl(method = "cv", 
                             number = 5,
                             verboseIter = TRUE,
                             classProbs = TRUE,
                             summaryFunction = twoClassSummary)

set.seed(1)
# tuning on 500 trees
rf.tuning.500 <- train(did.exceed ~ ., 
                       data = train1, 
                       method = "ranger", # fast implementation of a random forest: ranger, e1071 need to be installed
                       tuneGrid = grid, 
                       num.trees = 500,
                       trControl = tune.control,
                       importance = "impurity",
                       metric = "ROC")

# tuning on 1000 trees
rf.tuning.1000 <- train(did.exceed ~ ., 
                        data = train1 %>% select(-date), 
                        method = "ranger", 
                        tuneGrid = grid, 
                        num.trees = 1000,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")

# tuning on 1500 trees
rf.tuning.1500 <- train(did.exceed ~ ., 
                        data = train1 %>% select(-date), 
                        method = "ranger",
                        tuneGrid = grid, 
                        num.trees = 1500,
                        trControl = tune.control,
                        importance = "impurity",
                        metric = "ROC")


# parameter tuning results
rf.tuning.500$results # ROC: 0.9468062, Sens = 0.9687824, Spec = 0.7045118 --- mtry = 8, min.node.size = 5 0.8339352
rf.tuning.500$bestTune

rf.tuning.1000$results # ROC: 0.9506774, Sens = 0.9698187, spec = 0.6937374 --- mtry = 5, min.node.size = 5 0.8337254
rf.tuning.1000$bestTune

rf.tuning.1500$results # ROC: 0.9526248, Sens = 0.9729383, Spec = 0.7154209 --- mtry = 5, min.node.size = 5 0.8260119
rf.tuning.1500$bestTune

# assessing variable importance
varImp(rf.tuning.1500)
plot(varImp(rf.tuning.1500))
```

# Fitting model to the full training set

```{r}
final.grid <- data.frame(.mtry = 4,
                         .splitrule = "gini",
                         .min.node.size = 5)

final.control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

set.seed(1)
start.fit <- Sys.time()
rf.fit <- train(did.exceed ~ .,
                data = train1 %>% select(-date),
                method = "ranger",
                tuneGrid = final.grid,
                trControl = final.control,
                num.trees = 500,
                importance = "impurity",
                metric = "ROC")
end.fit <- Sys.time() # 1.4 seconds

# compute predictions
rf.preds <- predict(rf.fit, newdata = test, type = "prob")

rf.preds <- rf.preds %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(rf.preds$actual == rf.preds$prediction) # 0.9193382 accuracy

# ROC/AUC
roc.metric <- roc(predictions = rf.preds$yes,
                  labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric <- auc(roc.metric)

plot(roc.metric, main = paste("AUC:", auc.metric))
# sensitivity: true positive rate - ability of the model to correctly identify wind event days
# specificity: true negative rate - ability of the model to correctly identify non-wind event days

# assessing variable importance
plot(varImp(rf.fit))
varImp(rf.fit)

table(rf.preds$prediction, rf.preds$actual)
# precision = 138 / (16 + 138) = 0.896 # out of all that are being classified as true/yes - 89.6% are actually true/yes
# recall = 138 / (62 + 138) = 0.69 (out of all true observations, percentage of those that are being classified correctly)
```

# Predicting 24 hour pm10 concentration (not classification)

```{r}
# # setting up the data
# train2 <- training %>%
#   group_by(date.only) %>%
#   summarize(max.ws.cdf = max(ws.cdf, na.rm = TRUE),
#             max.wd.cdf = max(wd.cdf, na.rm = TRUE),
#             max.ws.s1 = max(ws.s1, na.rm = TRUE),
#             max.wd.s1 = max(wd.s1, na.rm = TRUE),
#             hour.max.wd.s1 = ifelse(length(which.max(wd.s1)) == 0, NA, which.max(wd.s1) - 1),
#             hour.max.ws.s1 = ifelse(length(which.max(ws.s1)) == 0, NA, which.max(ws.s1) - 1),
#             hour.max.wd.cdf = ifelse(length(which.max(wd.cdf)) == 0, NA, which.max(wd.cdf) - 1),
#             hour.max.ws.cdf = ifelse(length(which.max(ws.cdf)) == 0, NA, which.max(ws.cdf) - 1),
#             ws.in.range.cdf = sum((ws.cdf > 9), na.rm = TRUE),
#             wd.in.range.cdf = sum((wd.cdf > 288) & (wd.cdf < 320), na.rm = TRUE),
#             wd.in.range.s1 = sum((wd.s1 > 281) & (wd.s1 < 306), na.rm = TRUE),
#             ws.in.range.s1 = sum((ws.s1 > 8), na.rm = TRUE)) %>%
#   mutate(max.ws.cdf = ifelse(max.ws.cdf == -Inf, NA, max.ws.cdf)) %>%
#   mutate(max.wd.cdf = ifelse(max.wd.cdf == -Inf, NA, max.wd.cdf)) %>%
#   mutate(max.ws.s1 = ifelse(max.ws.s1 == -Inf, NA, max.ws.s1)) %>%
#   mutate(max.wd.s1 = ifelse(max.wd.s1 == -Inf, NA, max.wd.s1))
# 
# colnames(train2)[1] <- "date"
# 
# # merge train1 with other columns in cdf.master
# train2 <- train2 %>%
#   mutate(pm10.ave = pm10.averages$pm10.ave) %>%
#   left_join(cdf.master, by = "date") %>%
#   mutate(month = month(date)) %>%
#   mutate(day.of.month = day(date)) %>%
#   select(pm10.ave, ws.in.range.cdf, wd.in.range.cdf, wd.in.range.s1, 
#          ws.in.range.s1, max.ws.cdf, max.wd.cdf, max.ws.s1, max.wd.s1,
#          hour.max.wd.s1, hour.max.ws.s1, hour.max.wd.cdf, hour.max.wd.cdf, 
#          precip, s.rad, a.temp, rh, dp, s.temp, height, temp850, month, day.of.month)
# 
# train2 <- na.omit(train2) 
# 
# # =============================================================================================
# 
# grid2 <- expand.grid(.mtry = c(round(sqrt(ncol(train2))), 
#                                8, 10, 15),
#                      .splitrule = "variance",
#                      .min.node.size = c(5, 10, 20))
# 
# tune.control2 <- trainControl(method = "cv", 
#                              number = 5,
#                              verboseIter = TRUE,
#                              savePredictions = TRUE)
# 
# set.seed(1)
# # tuning on 500 trees
# rf.tuning.500.2 <- train(pm10.ave ~ ., 
#                          data = train2, 
#                          method = "ranger", # fast implementation of a random forest: ranger, e1071 need to be installed
#                          tuneGrid = grid2, 
#                          num.trees = 500,
#                          trControl = tune.control2,
#                          importance = "impurity")
# 
# # tuning on 1000 trees
# rf.tuning.1000.2 <- train(pm10.ave ~ ., 
#                           data = train2, 
#                           method = "ranger", 
#                           tuneGrid = grid2, 
#                           num.trees = 1000,
#                           trControl = tune.control2,
#                           importance = "impurity")
# 
# # tuning on 1500 trees
# rf.tuning.1500.2 <- train(pm10.ave ~ ., 
#                           data = train2, 
#                           method = "ranger",
#                           tuneGrid = grid2, 
#                           num.trees = 1500,
#                           trControl = tune.control2,
#                           importance = "impurity")
# 
# rf.tuning.500.2$bestTune # mtry = 8, min.node.size = 5 RMSE: 12.16776  r2: 0.8233791  MAE: 8.976453 ****
# rf.tuning.1000.2$bestTune # mtry = 10, min.node.size = 5 RMSE: 12.07823  r2: 0.8215156  MAE: 8.931860
# rf.tuning.1500.2$bestTune # mtry = 8, min.node.size = 5 RMSE: 11.95274 r2: 0.8239232  MAE: 8.873622
# 
# # =============================================================================================
# 
# # defining new metric:
# # if the difference between prediction and actual concentration is > x, classify as incorrect ? 
# # metric won't rely on the probability threshold - would depend on how close prediction is to actual value
# 
# preds <- rf.tuning.500.2$pred %>%
#   filter(mtry == 8) %>%
#   filter(min.node.size == 5) %>%
#   mutate(residual = pred - obs)
# 
# ggplot(preds, aes(x = residual)) +
#   geom_histogram()
# 
# sum(abs(preds$residual) > 15) / nrow(preds) # 17% of predictions have residuals greater than 15
```

# Comparing random forest to regular classification tree

```{r}
treefit <- tree(did.exceed ~ ., data = train1 %>% select(-date), split = "gini" )
summary(treefit)

# CV to determine optimal size of tree
cv.treefit <- cv.tree(treefit, FUN = prune.misclass)
cv.treefit
plot(cv.treefit) # size = 6 or 7 did best

pruned.tree <- prune.misclass(treefit, best = 6)
summary(pruned.tree) #  0.09636 = 119 / 1235 
plot(pruned.tree)
text(pruned.tree)

# re-train again, using only variables in pruned tree 
vars <- unique(as.character(pruned.tree$frame[ , 1]))
vars <- vars[-which(vars == "<leaf>")]
new.train <- train1[ , c("did.exceed", vars)]

new.tree <- tree(did.exceed ~ ., data = new.train, split = "gini")
summary(new.tree) #  0.08259 = 102 / 1235

# predict on test data 
tree.preds <- predict(new.tree, newdata = test) %>% 
  as.data.frame() %>%
  mutate(prediction = ifelse(yes > 0.50, "yes", "no")) %>%
  mutate(actual = test$did.exceed)

# classification accuracy
mean(tree.preds$actual == tree.preds$prediction) 

# ROC/AUC
roc.metric.tree <- roc(predictions = tree.preds$yes,
                       labels = as.factor(ifelse(as.character(test$did.exceed) == "yes", 1, 0)))
auc.metric.tree <- auc(roc.metric.tree)
plot(roc.metric.tree, main = paste("AUC:", auc.metric.tree))

# confusion matrix
table(tree.preds$prediction, tree.preds$actual)
# precision = 127 / (34 + 127) = 0.789 # out of all that are being classified as true/yes - 78.9% are actually true/yes
# recall = 127 / (73 + 127) = 0.63.5 (out of all true observations, percentage of those that are being classified correctly)
```


```{r}
# COMPARISON 
# ==========================
# classification accuracies:
# ==========================
# tree
mean(tree.preds$actual == tree.preds$prediction) # 0.8893485
# random forest
mean(rf.preds$actual == rf.preds$prediction)

# ========
# ROC/AUC
# ========
# tree
plot(roc.metric.tree, main = paste("AUC:", auc.metric.tree))
# random forest
plot(roc.metric, main = paste("AUC:", auc.metric))
```
























